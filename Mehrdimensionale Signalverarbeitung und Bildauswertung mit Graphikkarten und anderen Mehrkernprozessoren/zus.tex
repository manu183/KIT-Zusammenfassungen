\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage[ngerman]{babel}
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[section]{placeins}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{float}
\usepackage{multicol}
\usepackage{varwidth}
\usepackage{sectsty} % Allows customizing section commands
\usepackage{enumitem}
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize
\textsc{Karlsruhe Institute of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Mehrdimensionale Signalverarbeitung und Bildauswertung mit Graphikkarten und anderen Mehrkernprozessoren % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Manuel Lang} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title
\newpage
\tableofcontents
\newpage

\section{Introduction}

\begin{itemize}
  \item Parallel computing: Use of multiple (interacting) computational units(CU) to execute a (divisible) task.
  \item Amdahls law: We want to know how fast we can complete a task on a particular data set by increasing the CU count. $\eta_n = \frac{TW}{T(n)W} = \frac{T}{t_s + \frac{t_p}{n} + t_{comm}}$
  \begin{itemize}
    \item $\eta_n$: Speeup
    \item $W$: Work load
    \item $T$: Total runtime $t_s + t_p$
    \item $t_s$: Runtime serial part
    \item $t_p$: Runtime parallel part
    \item $t_{comm}$: Communication time (normally not included)
    \item $n$: Number of computational units
  \end{itemize}
  \item Gustafson's law: We want to know if we can analyze more data in approximately the same amount of time by increasing the CU count. $\eta_n = \frac{TW(n)}{TW} = (1-p)W + npW$
  \begin{itemize}
    \item $\eta_n$: Speeup
    \item $W$: Work load
    \item $T$: Total runtime
    \item $p$: Workload fraction benefiting from additional CUs
    \item $W(n): (1-p)W + np W \equiv aW + n(1-a)W = nW - a(n-1)W$
    \item $n$: Number of computational units
  \end{itemize}
  \item Data parallelism: Each CU performs the same task on dierent data
  \begin{itemize}
    \item The CPU cores and GPU streaming-cores are OpenCL compute devices
    \item Concurrent processing on all herterogeneous cores.
  \end{itemize}
  \item Task parallelism: Each CU performs a different task on the same data
  \item Instruction level parallelism: Automatic parallel execution of instructions by processor
  \item Spatial parallelism: More units work in parallel
  \item Temporal parallelism $\rightarrow$ Pipelining
  \item Latency
  \begin{itemize}
    \item The latency of an instruction is the \textbf{delay} that the instruction generates in a dependy chain. The measurement unit is clock cycles.
    \item CPUs try to minimize latency. Low efficiency on parallel portions.
  \end{itemize}
  \item Throughput
  \begin{itemize}
    \item The throughput is the maximum number of instructions of the same kind that can be executed per clock cycle when the operands of each instruction are independant of the preceding instrucions.
    \item GPUs try to maximize throughput. Low performance on sequentiel portions.
  \end{itemize}
  \item Graphic card slang
  \begin{itemize}
    \item A GPU executes a program, the \textit{kernel}.
    \item A thread executes an instance of the kernel.
    \item Threads are combined into warps/wavefronts running in lockstep. Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently.
    \item Warps/wavefronts are part of threaded blocks/work groups. These are defined by the user.
    \item A thread runs on a core. A number of cores form a \textit{Streaming Multiprocessor (SM) / Compute Unit (CU)}. Thread blocks/work groups are scheduled over SMs/CUs.
  \end{itemize}
\end{itemize}

\section{Parallelism / Programming models}

\begin{itemize}
  \item Foster's PCAM model
  \begin{itemize}
    \item Tasks communicate over channels
    \begin{itemize}
      \item Task: A program with local memory, in- and outports. Tasks execute concurrently. The number of tasks can vary during program execution.
      \item Channel: Connecting outports of a task with import of another task. Channels are buffered sending asynchronously and receiving synchronously (task blocks).
    \end{itemize}
    \item Paritioning: The computation that is to be performed and the data operated on by this computation are decomposed into small tasks. Practical issues such as the number of processors in the target computer are ignored, and attention is focused on recognizing opportunities for parallel execution.
    \begin{itemize}
      \item The partitioning stage of a design is intended to expose opportunities for parallel execution.
      \item Focus is on defining a large number of small tasks (fine-grained decomposition).
      \item A good partition divides both the computation and the data into small pieces.
      \item One approach is to focus first on partitioning the data associated with a problem; this is called domain decomposition.
      \begin{itemize}
        \item First partition data; ideally divide data into small pieces of approximately equal size.
        \item Next partition computation, typically by associating each operation with the data on which it operates.
        \item Focus first on the largest data structure or on the data structure that is accessed most frequently.
      \end{itemize}
      \item The alternative approach, termed functional decomposition, decomposes the computation into seperate tasks before considering how to partition the data.
      \begin{itemize}
        \item Initital focus is on the computation that is to be performed rather than on the data.
        \item Divide computation into disjoint tasks.
        \item Examine data requirements of tasks:
        \begin{enumerate}
          \item Requirements may be disjoint, in which case the partition is complete.
          \item Requirements may overlap significantly, in which case considerable communication will be required to avoid replication of data. Domain decomposition should be considered instead.
        \end{enumerate}
        \item Functional decomposition is valuable as a different way of thinking about problems and should be considered when exploring possible parallel algorithms.
        \item A focus on the computations that are to be performed can sometimes reveal structure in a problem, and hence opportunities for optimization, that would not be obvious from a study of data alone.
        \item Functional decomposition is an important program structuring technique; can reduce the complexity of the overall design.
      \end{itemize}
      \item Theses are complementary techniques.
      \item Seek to avoid replicating computation and data (may change this later in process).
      \item Paritioning checklist
      \begin{itemize}
        \item Does your partition define at least an order of magnitude more tasks than there are processors in your target computer?
        \item Does your partition avoid redundant computation and storage requirements?
        \item Are tasks of comparable size?
        \item Have you identified several alternative partitions?
      \end{itemize}
    \end{itemize}
    \item Communication: The communication required to coordinate task execution is determined, and appropriate communication structures and algorithms are defined.
    \begin{itemize}
      \item Conceptualize a need for communication between two tasks as a channel linking the tasks, on which one task can send messages and from which the other can receive.
      \item Channel structure links tasks that require data (consumers) with tasks that possess those data (producers).
      \item Definition of a channel involves an intellectual cost and the sending of a message involves a physical cost - avoid introducing unnecessary channels and communication operations.
      \item We want to distribute communication operations over many tasks.
      \item We want to organize communication operations in a way that permits concurrent execution.
      \item For functional decomposition communication is often 'natural'.
      \item For domain decomposition:
      \begin{itemize}
        \item First partition data structures into disjoint subsets and then associate with each datum those operations that operate solely on that datum.
        \item Handle dependencies with other tasks.
      \end{itemize}
      \item Communication patterns
      \begin{itemize}
        \item local vs. global
        \item structured vs. unstructured
        \item static vs. dynamic
        \item synchronous vs. asynchronous
      \end{itemize}
      \item Communication checklist
      \begin{itemize}
        \item Do all tasks perform about the same number of communication operations?
        \item Does each task communicate only with a small number of neighbors?
        \item Are communication operations able to proceed concurrently?
        \item Is the computation associated with different tasks able to proceed concurrently?
      \end{itemize}
    \end{itemize}
    \item Agglomeration: The task and communication structures defined in the first two stages of a design are evaluated with respect to performance requirements and implementation costs. If necessary, tasks are combined into larger tasks to improve performance or to reduce development costs.
    \begin{itemize}
      \item Move from parallel abstractions to real implementation. At this point we've broken down our problem enough that we understand the individual tasks and the necessary communication between tasks. The goal now is to being making the parallel solution practical and as efficient as possible.
      \begin{itemize}
        \item Is it useful to combine, or agglomerate, tasks to reduce the number of tasks?
        \item Is it worthwile to replicate data and/or computation?
      \end{itemize}
      \item Agglomeration goals
      \begin{itemize}
        \item Reducing communication costs by increasing computation and communication granularity.
        \item Retaining flexibility with respect to scalability and mapping decisions.
      \end{itemize}
      \item Surface-to-Volume effects
      \begin{itemize}
        \item The communication requirements of a task are proportional to the surface of the subdomain on which it operates, while the computation requirements are proportional to the subdomain's volume. In a two-dimensional problem, the surface scales with the problem size while the volume scales as the problem size squared.
        \item The amount of communication performed for a unit of computation (the communication/computation ratio) decreases as task size increases. This effect is often visible when a partition is obtained by using domain decomposition techniques.
      \end{itemize}
      \item Replication of computations: Sometimes it's more efficient for a task to compute a needed quantity rather than to receive it from another task where it is already known or has been computed.
      \item Avoid communication: Agglomeration is almost always beneficial if analysis of communication requirements reveals that a set of tasks cannot execute concurrently.
      \item Preserving flexibility
      \begin{itemize}
        \item It is important when agglomerating to avoid making design decisions that limit unnecessarily an algorithm's scalability. For example, we might choose to decompose a multidimensional data structure in just a single dimension.
        \item Don't assume during the design that the number of processors will always be limited to the currently available number.
        \item Good parallel algorithms are designed to be resilient to changes in processor count.
        \item It can be advantageous to map several tasks to a processor. Then, a blocked task need not result in a processor becoming idle, since another task may be able to execute in its place.
      \end{itemize}
      \item Agglomeration checklist
      \begin{itemize}
        \item Has agglomeration reduced communication costs by increasing locality?
        \item If agglomeation has replicated communication, have you verified that the benefits of this replication outweigh its costs, for a range of problem sizes and processor counts?
        \item If agglomeration replicates data, have you verified that this does not compromise the scalability of your algorithm by restricting the range of problem sizes or processor counts that it can address?
        \item Has agglomeration yielded tasks with similar computation and communication costs? The larger the tasks created by agglomeration, the more important it is that they have similar costs.
        \item Does the number of tasks still scale with problem size?
        \item If agglomeration eliminated opportunities for concurrent execution, have you verified that there is sufficient concurrency for current and future target computers?
        \item Can the number of tasks be reduced still further, without introducing load imbalances or reducing scalability?
      \end{itemize}
    \end{itemize}
    \item Mapping: Each task is assigned to a processor in a manner that attempts to satisfy the competing goals of maximizing processor utilization and minimizing communication costs. Mapping can be specified statically or determined at runtime by load-balancing algorithms.
    \begin{itemize}
      \item At this point we have a set of tasks and we need to assign them to processors on the available machine. The mapping problem does not arise on uniprocessors or on shared-memory computers that provide automatic task scheduling.
      \item General-purpose mapping mechanisms have yet to be developed for scalable parallel computers. The general-case mapping problem is NP-complete.
      \item Our goal in developing mapping algorithms is normally to minimize total execution time. We use two strategies to achieve this goal:
      \begin{enumerate}
        \item We place tasks that are able to execute concurrently on different processors, so as to enhance concurrency.
        \item We place tasks that communicate frequently on the same processor, so as to increase locality.
      \end{enumerate}
      \item Considerable knowledge has been gained on specialized strategies and heuristics and the classes of problem for which they are effective.
      \item When domain decomposition is used there is often a fixed number of equal-size tasks and structured local and global communication.
      \item If, instead, there are variable amounts of work per task and/or unstructured communication patterns, we might use load balancing algorithms that seek to identify efficient agglomeration and mapping strategies. The time required to execute these algorithms must be weighed against the benefits of reduced execution time. Probabilistic load-balancing methods tend to have lower overhead than do methods that exploit structure in an application.
      \item When either the number of tasks or the amount of computation or communication per task changes dynamically during program execution we might use dynamic load-balancing strategy in which a load-balancing algorithm is executed periodically to determine a new agglomeration and mapping.
      \item If functional decomposition is used we can use task-scheduling algortihms which allocate tasks to processors that idle or that are likely to become idle.
    \end{itemize}
  \end{itemize}
  \item Parallel Patterns
  \begin{itemize}
    \item The Berkeley View
    \begin{itemize}
      \item Seven critical questions for 21st century parallel computing
      \begin{itemize}
        \item Applications
        \begin{itemize}
          \item What are the applications?
          \item What are common kernels of the applications?
        \end{itemize}
        \item Hardware
        \begin{itemize}
          \item What are the hardware building blocks?
          \item How to connect them?
        \end{itemize}
        \item Programming models
        \begin{itemize}
          \item How to describe applications and kernels?
          \item How to program the hardware?
        \end{itemize}
        \item Evaluation
        \begin{itemize}
          \item How to measure success?
        \end{itemize}
      \end{itemize}
      \item 13 Dwarfs
      \begin{enumerate}
        \item Finite State Machine
        \item Combinational Logic
        \item Graph Traversal
        \item Structured Grids
        \item Dense Linear Algebra
        \item Sparse Linear Algebra
        \item Spectral Methods (FFT)
        \item Dynamic Programming
        \item N-Body Methods
        \item MapReduce
        \item Back-track/Branch \& Bound
        \item Graphical Model Inference
        \item Unstructured Grids
      \end{enumerate}
      \item 4 Valuable Roles of Dwarfs
      \begin{enumerate}
        \item "Anti-benchmarks" not tied to code or language artifcats $\implies$ encourage innovation in algorithms, languages, data structures, and/or hardware
        \item Universal, understandable vocabulary to talk across disciplinary boundaries
        \item Define building blocks for creating libraries \& frameworks that cut across app domains
        \item They decouplse research, allowing analysis of HW \& SW engineering and programming without waiting years for full apps
      \end{enumerate}
      \item Parallel Patterns
      \begin{itemize}
        \item A recurring combination of tasks distribution and data access that solves a specific problem in parallel algorithm design.
        \item Patterns can be used to organize your code, leading to algorithms that are most scalable and maintainable.
        \item Patterns are universal - they can be used in any parallel programming system.
        \item Good parallel programming models support a set of useful parallel patterns with low-overhead implementations.
      \end{itemize}
      \item Serial Control Patterns
      \begin{itemize}
        \item Sequence: A sequence is a ordered list of tasks that are executed in a specific order. Each task is completed before the one after it starts.
        \item Selection: In the selection pattern, a condition c is evaluated first. If the condition is true, some task a is executed. If c is false, task b will be executed. There is a control-flow dependency between c and a,b, so neither a or b is executed before c. Exactly one of a or b will be executed, never both.
        \item Iteration: In the iteration pattern, a condition c is evaluated. If it is true, a task a is evaluated, then c is evaluated again, and the process repeats until the condition becomes false. The number of iterations is data dependend.
        \item Recursion: Recursion is a dynamic form of nesting which allows functions to call themselves, directly or indirectly.
      \end{itemize}
      \item Parallel Control Patterns
      \begin{itemize}
        \item Fork-Join: The fork-join pattern lets control flow fork into multiple parallel flows that rejoin later.
        \item Map: The map pattern replicates a (elemental) function over every element of an index set. The index set may be abstract or associated with the elements of a acollection.
        \item Stencil: Stencil applies a function to neighbourhoods of a collection.
        \item Reduction: Combines every element in a collection using an associative "combiner function". Because of the associativity of the combiner function, different orderings of the reduction are possible.
        \item Scan: Computes all partial reduction of a collection. For every output in a collection, a reduction of the input up to that point is computed. If the function being used is associative, the scan can be parallelized. Parallelizing a scan is not obvious at first, because of dependencies to previous iterations in the serial loop. A parallel scan will require more operations than a serial version.
        \item Recurrence: Recurrence results from loop nests with both input and output dependencies between iterations. For a recurrence to be computable, there must be a serial ordering of the recurrence elements so that elements can be computed using previously computed outputs.
      \end{itemize}
      \item Serial Data Management Patterns
      \begin{itemize}
        \item Random Read and Write
        \item Stack Allocation
        \item Heap Allocation
        \item Closures
        \item Objects
      \end{itemize}
      \item Parallel Data Management Patterns
      \begin{itemize}
        \item Pack: Pack is used eliminate unused space in a collection. Elements marked false are discarded, the remaining elements are placed in a contiguous sequence in the same order. Useful when used with map.
        \item Pipeline: A Pipeline connects tasks in a producer-consumer manner. Some stages may retain state. Pipelines are most useful when used with other patterns as they can multiply available parallelism.
        \item Geometric Decomposition: The geometric decomposition arranges data into subcollections. Overlapping and non-overlapping decompositions are possible.
        \item Gather: Gather reads a collection of data given a collection of indices. The output collection shares the same type as the input collection, but it share the same shape as the indices collection.
        \item Scatter: Scatter is the inverse of gather. A set of input and indices is required, but each element of the input is written to the output at the given index instead of read from the input at the given index. Race conditions can occur when we have two writes to the same location.
      \end{itemize}
      \item Other parallel patterns
      \begin{itemize}
        \item Workpile: general map pattern where each instance of elemental function can generate more instances, adding to the "pile" of work.
        \item Search: finds some data in a collection that meets some criteria.
        \item Segmentation: operations on subdivided, nonoverlapping, non-uniformly sized partitions of 1D collections.
        \item Expand: a combination of pack and map. Each map can output any number of elements. The outputs are packed in a specific order.
        \item Category Reduction: Given a collection of elements each with a label, find all elements with same label and reduce them.
      \end{itemize}
    \end{itemize}
    \item The Intel View
  \end{itemize}
\end{itemize}

\section{OpenMP}

\begin{itemize}
  \item Supporting Compilers: gcc, PGI, clang, Intel Compiler
  \item based on a shared memory model
  \item utilizes software threads (OpenMP runtime system)
  \item Program use by API, pragmas and Environment variables
  \item Supported Languages: C, C++, Fortran
  \item Threads
  \begin{itemize}
    \item Concept: Fork-Join
    \item OpenMP programs start with just one thread: The Master
    \item Worker threads are spawned at Parallel Regions. Together with the Master they form a Team.
    \item In between Parallel Regions the Worker threads are put to sleep.
    \item Allows for an incremental parallelization!
  \end{itemize}
  \item OpenMP Parallel Regions
  \begin{itemize}
    \item A parallel region is a block of code that will be executed by multiple threads. This is the fundamental OpenMP parallel construct.
    \item When a thread reaches a PARALLEL directive, it creates a team of threads and becomes the master of the team. The master is a member of that team and has thread number 0 within that team.
    \item Starting from the beginning of this parallel region, the code is duplicated and all threads will execute that code.
    \item There is an implied barrier at the end of a parallel region. Only the master thread continues execution past this point.
    \item If any thread terminates within a parallel region, all threads in the team will terminate, and the work done up until that point is undefined.
  \end{itemize}
  \item OpenMP Thread number
  \begin{itemize}
    \item The number of threads in a parallel region is determined by the following factors, in order of precedence:
    \begin{enumerate}
      \item Evaluation of the $IF$ clause
      \item Setting of the $NUM_THREADS$ clause
      \item Use of the $omp_set_num_threads()$ library function
      \item Setting of the $OMP_NUM-THREADS$ environment variable
      \item Implementation default - usually the number of CPUs on a node, thought it could be dynamic
    \end{enumerate}
    \item Threads are numbered from 0 (master thread) to N-1
  \end{itemize}
  \item OpenMP Work-Sharing Constructs
  \begin{itemize}
    \item for Construct: The for directive identifies an iterative work-sharing construct that specifies that the iterations of the associated loop will be executed in parallel. The iterations of the for loop will be executed in parallel. The iterations of the for loop are distributed across threads that already exist in the team executing the parallel construct to which it binds.
    \item The sections directive identifies a noniterative work-sharing construct that specifies a set of constructs that are to be divided among threads in a team. Each section is executed once by a thread in the team. The syntax of the sections directive is as follows... TODO
    \item The single directive identifies a construct that specifies that the associated structured block is executed by only one thread in the team (not necessarily the master thread). The syntax of the single directive is as follows... TODO
  \end{itemize}
  \item OpenMP Memory Model
  \begin{itemize}
    \item All threads have access to the same, globally shared memory
    \item Data in private memory is only accessible by the thread owning this memory
    \item No other thread sees the change(s) in private memory
    \item Data transfer is through shared memory and is 100\% transparent to the application
    \item Private data is undefined on entry and exit (Can use firstprivate and lastprivate to address this)
    \item Each thread has its own temporary view on the data
  \end{itemize}
  \item OpenMP Data Sharing
  \begin{itemize}
    \item shared: #pragma omp parallel for shared(a)
    \item private: #pragma imp parallel for private(a) - a private copy is an uninitialized variable by the same name and same type as the original variable; it does not copy the value of the variable that was in the surrounding context.
    \item firstprivate: #pragma omp parallel for firstprivate(a) - a firstprivate copy is an initialized variable by the same name and same type as the original variable; it does copy the value of the variable that was in the surrounding context.
    \item lastprivate: #pragma omp parallel for lastprivate(a) - The lastprivate clause defines a variable private as in firstprivate or private, but causes the value from the last task to be copied back to the original value after the end of the loop/sections construct. In a loop construct (for construct), the last value is the value assigned by the thread that handles the last iteration of the loop. Values assigned during other iterations are ignored.
  \end{itemize}
  \item OpenMP Synchronization
  \begin{itemize}
    \item Data Race: If between two synchronization points at least one thread writes to a memory location from which at least one other thread reads, the result is not deterministic (race condition).
  \end{itemize}
  \item OpenMP Critical Region: The $CRITICAL$ directive specifies a region of code that must be executed by only one thread at a time. If a thread is currently executing inside a $CRITICAL$ region and another reaches that $CRITICAL$ region and attempts to execute it, it will block until the first thread exists that $CRITICAL$ region.
  \item OpenMP Atomic: The $ATOMIC$ directive specifies that a specific memory location must be updated atomically, rather than letting multiple threads attempt to write to it. In essence, this directive provides a mini-CRITICAL section. The directive provides a mini-CRITICAL section. The directive applies only to a single, immediately following statement.
  \item OpenMP Reduction: The reduction clause is a mix between the private, shared, and atomic clause. It allows to accumulate a shared variable without the atomic clause, but the type of accumulation must be specified. It will often produce faster executing code than by using the atomic clause.
  \item OpenMP Barrier: The barrier directive causes threads encountering the barrier to wait until all the other threads in the same team have encountered the barrier. There is an implicit barrier at the end of each parallel block, and at the end of each sections, for and single statement, unless the nowait directive is used.
  \item OpenMP Task: In OpenMP, an explicit task is specified using the task directive. The task directive defined the code associated with the task and its data environment. The task construct can be placed anywhere in the program; whenever a thread encounters a task construct, a new task is generated.
  \item OpenMP Devices
  \begin{itemize}
    \item device: An implementation defined logical execution engine. (A device could have one or more processors.)
    \item host device: The device on which the OpenMP program begins execution.
    \item target device: A device onto which code and data may be offloaded from the host device. It is implementation defined (i.e. optional).
  \end{itemize}
  \item OpenMP Device Execution Model
  \begin{itemize}
    \item host-centric; host device offloads target regions (code and data) to target devices
    \item the target region may be executed by the target device or host device
    \item the target region is executed by the host device if a rget device does not exist or the target device is not supported by the implementation or the target device cannot execute the target construct
    \item the task that encounters the target construct waits at the end of the construct until execution of the region completes
    \item a target region is executed on one target device
    \item each device has its own threads
  \end{itemize}
  \item OpenMP Device Data Model: Creates a device data environment
  \begin{itemize}
    \item data can be implicitly mapped to the target region
    \item data can explicitly (via data-mapping attribute clause) mapped to the target region
    \begin{itemize}
      \item provide more precise information to the compiler
      \item reduce unnecessary data transfer
    \end{itemize}
    \item device: specify on which device the data environment is created
    \item map:
    \begin{itemize}
      \item alloc each new corresponding list item has an undefined initial value
      \item to each new corresponding list item is initialized with the original list item's value
      \item from on exit from the region the corresponding list item's value is assigned to each original list item
      \item fromto default
    \end{itemize}
    \item if: if present and false, the device is the host
  \end{itemize}
  \item OpenMP Offloading: TODO
  \item OpenMP target vs target data
  \begin{itemize}
    \item The target directive transfers the control flow to the target device. The map clause controls the data transfer directions.
    \item The targetdata directive creates a scoped devices data environment. The map clause controls the data transfer directions. The device data environmen is valid during the lifetime of the region.
    \item Device data environment can be used to keep data on target
  \end{itemize}
  \item OpemMP declare target
  \begin{itemize}
    \item a declarative directive
    \item specify variables and procedures being mapped to a device
    \item this directive instructs the compiler that the functions are called or the variables are referenced in the target region; a target device version of the code needs to be generated
  \end{itemize}
  \item OpenMP Device Teams
  \begin{itemize}
    \item the teams construct creates a league of thread teams
    \item by default, one team is created
    \item exploit extra level of parallelism on some hardware (e.g. Nvidia GPU)
    \item work can be distributed among the teams (distribute construct)
  \end{itemize}
  \item OpenMP distribute
  \begin{itemize}
    \item iterations distributed among master threads of all teams
    \item specify to the loops only
    \item must be closely nested to the teams construct
    \item workshare among teams to exploit the parallelism on the target device
    \item the iterations are divided into blocks, each block is offloaded to the teams regions
    \item collapse(N): Turns the next N loops into one, linearized loop
    \item dist_schedule(kind[, chunk_size]): If dist_schedule is specified, kind must be static. If specified, iterations are divided into chunks of size chunk_size, chunks are assigned to the teams of the league in a round-robin fashion in the order of the team number.
  \end{itemize}
  \item OpenMP simd
  \begin{itemize}
    \item The simd construct can be applied to a loop to indicate that the loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD instructions).
    \item safelen(N): no two iterations executed concurrently with SIMD instructions can have a greater distance in the logical iteration space than N, where N is an integer constant
    \item simdlen(N): If used, the simdlen clause specifies the preferred number of iterations to be executed concurrently. The parameter of the simdlen clause must be a constant positive integer.
    \item linear: The linear clause declares one or more list items to be private to a SIMD lane and to have a linear relationship with respect to the iteration space of a loop.
  \end{itemize}
\end{itemize}

\section{OpenACC}

\begin{itemize}
  \item Supporting Compilers: gcc, PGI
  \item Similiar to OpenMP, differences: in OpenMP parallelism is expecitly expressed by the user. Correctness must be provided by the user. In OpenACC parallelism can be expressed implicitly or explicitly.
  \item A kernel is a function that runs in parallel on the GPU.
  \item When a program encounters a kernels construct, it will launch a sequence of kernels in order on the device.
  \item The kernels directive expresses that a region may contain parallelism and the compiler determines what can be safely parallelized.
  \item The compiler breaks code in the kernel region into a sequence of kernels for execution on the accelerator device.
  \item The parallel directive identifies a block of code as having parallelism.
  \item Compiler generates a parallel kernel for that loop.
  \item The loop directive is used within a parallel or kernels directive identifying a loop that can be executed on the accelerator device.
  \item The loop directive can be combined with the conclosing parallel or kernels
  \item OpenACC loop clauses
  \begin{itemize}
    \item gang: partition the loop across gangs
    \item worker: partition the loop across workers
    \item vector: vectorize the loop
    \item seq: do not partition this loop, run it sequentially instead
  \end{itemize}
  \item OpenACC loop restrictions: The OpenACC specifications enforces that the outermost loop must be a gang loop, the innermost parallel loop must be a vector loop, and a worker loop may appear in between. A sequential loop may appear at any level. When using the parallel directive, one can partition the loop in the form of the num_gangs, num_workers, and vector_length clauses to the parallel directive
  \item copy(list): Allocates memory on GPU and copies data from host to GPU when entering region and copies data to the host when exiting region.
  \item copyin(list): Allocates memory on GPU and copies data from host to GPU when entering region.
  \item copyout(list): Allocates memory on GPU and copies data to the host when exiting region.
  \item create(list): Allocates memory on GPU but does not copy.
  \item present(list): Data is already present on GPU from another containing data region.
\end{itemize}

\section{OpenCL}

\begin{itemize}
  \item Intel Sandybridge Architecture
  \begin{itemize}
    \item Sandybridge has 6 cores and 12 hardware threads (SMT)
    \item Floating point units can operate on 4 (DP) pr 8 (SP) data words simultaneously in a SIMD fashion.
  \end{itemize}
  \item Nvidia Fermi Architecture
  \begin{itemize}
    \item Fermi has 16 streaming multiprocessors (SM)
    \item Each SM has 32 cores such that 512 cores reside within the Fermi
    \item Each core has an arithmetic logic and a floating point unit
  \end{itemize}
  \item CPU and GPU Specs
  \begin{itemize}
    \item GPUs exhibit a vast number of arithmetic logic and floating point units which exceeds the number of working units of CPUs
    \item CPUs are used for more general work and provide more caches and control logic for hiding latencies of memory transfers
    \item GPUs tackle this bottleneck with thread-level parallelism applied on huge data sets.
  \end{itemize}
  \item Parallel Programming
  \begin{itemize}
    \item Need to fully utilizt all computational resources
    \item In many cases compilers are unable to achieve the desired efficiency
    \item CPUs
    \begin{itemize}
      \item Cilk (spawn, sync, inlet, abort)
      \item High Performance Fortran (forall)
      \item C++ 11 Thread library (async, join, ...)
      \item Thread Building Blocks (parallel_for, parallel_do, ...)
      \item OpemMP (# pragma omp parallel)
      \item Message Passing Interface
    \end{itemize}
    \item GPUs
    \begin{itemize}
      \item Shader Programming (OpenGL, Direct3D)
      \item Rapidmind
      \item Brook+
      \item Nvidia Cuda
    \end{itemize}
  \end{itemize}
  \item OpenCL Key Features
  \begin{itemize}
    \item open and royalty-free standard for parallel programming
    \item portable and efficient access to heterogeneous processing platforms
    \item enhances significantly enhanced parallel programming productivity
  \end{itemize}
  \item Hierarchy of Models for Abstraction
  \begin{itemize}
    \item Platform model: host - device - compute units
    \item Execution model: command queues - work items / groups
    \item Memory model: regions - objects - consistency
    \item Programming model: platform and runtime layer - kernels
  \end{itemize}
  \item Programming Models
  \begin{itemize}
    \item Task parallelism - OpenCL API
    \begin{itemize}
      \item Within the host program with C or C++
      \item Decompose program into smaller function units
      \item Let the function units run in parallel
    \end{itemize}
    \item Data parallelism - OpenCL-C/C++
    \begin{itemize}
      \item Within separate functions kernels with a dialect of C99 or C++14
      \item Decompose data into smaller data sets
      \item Let functions or program segments process the data sets in parallel
    \end{itemize}
  \end{itemize}
  \item Host \& Platforms
  \begin{itemize}
    \item A Host conducts how platforms are utilized
    \item Host programs utilize the OpenCL API
    \item A Platform is a collection of heterogeneous devices
    \item Number of platforms depends on the vendors
    \item Communication between Platforms over host
  \end{itemize}
  \item Plaform \& Devices
  \begin{itemize}
    \item Each platform may consist of heterogeneous devices
    \item Each device can have different number of compute units
    \item Each compute unit (CU) has the same amount of processing elements
    \item Every device is programmed using an extended C dialect
  \end{itemize}
  \item Device \& Subdevice
  \begin{itemize}
    \item Device is a hierarchical shared memory system
    \begin{itemize}
      \item Processing elements (PE) share a fast local memory within a compute unit
      \item Slower global memory is shared across all processing elements within a device
    \end{itemize}
    \item PEs communicate and synchronize within a CU faster than across CUs
    \item Devices might be equally subdivded
  \end{itemize}
  \item Context \& Device
  \begin{itemize}
    \item A Context can be regarded as a collection of devices
    \item Contexts are created for one or multiple devices within a platform
    \item Contexts are used by the OpenCL runtime for managing objects
  \end{itemize}
  \item Context \& Command Queue
  \begin{itemize}
    \item A command queue is processed on a single device to execute commands
    \item Each context may contain several queues
    \item Commands from different queues run independently from each other
    \item Barriers and markers for synchronization
  \end{itemize}
  \item Context \& Objects
  \begin{itemize}
    \item OpenCL objects are created using a single context
    \item Operations on these objects are performed using a queue
    \item Objects do not see devices!
    \item There are different object types
    \begin{itemize}
      \item Buffer(Memory) objects
      \begin{itemize}
        \item A Buffer is an one dimensional collection of elements on which data parallel operations are performed.
        \item The data type of a buffer is specified by the corresponding argument type of the OpenCL kernel function
        \item Read and write operations are performed using command queues which must be within the same context
      \end{itemize}
      \item Event objects
      \item Program objects
      \begin{itemize}
        \item Programs hold data parallel code which are referred to as kernels
        \item Kernels can be either strings or can reside in a file
        \item Programs may be built for specified devices residing in the same context
        \item The build process includes the creating of OpenCL kernels
        \item Build options available such as optimizations, preprocessor directives
        \item programs run only on devices for which they were built
      \end{itemize}
      \item Kernel objects
      \begin{itemize}
        \item OpenCL kernels are data parallel functions written in the OpenCL C language
        \item Kernel objects are created by program objects which contain the corresponding OpenCL kernels
        \item No changes allowed to the OpenCL kernel functions after the build process
        \item Kernel can be run after specifying its arguments
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Task-Level Parallelism with the OpenCL-API}

\begin{itemize}
  \item Different Command Types for
  \begin{itemize}
    \item executing kernels, e.g. clEnqueueNDRangeKernel
    \item Executing memory ops, e.g. clEnqueueReadBuffer
    \item synchronization, e.g. clEnqueueMarkerWithWaitList
  \end{itemize}
  \item Commands can be
  \begin{itemize}
    \item blocking: API function do not return until command has completed (memory read/write, flushing)
    \item non-blocking: API function immediately returns -> execute asynchronous to the host thread
  \end{itemize}
  \item Command Queues
  \begin{itemize}
    \item Command Queue Execution Types
    \begin{itemize}
      \item in-order: appear to execute commands in the order of queuing
      \item out-of-order: execute commands in any order
    \end{itemize}
    \item Command Queue States
    \begin{itemize}
      \item queued: command within the queue
      \item submitted: flushed and submitted for execution
      \item ready: all prerequisites for execution have been met
      \item running: command is running on the specified device
      \item ended: execution of commands end: all workgroups ended
      \item complete: child commands ended
    \end{itemize}
  \end{itemize}
  \item Event Objects
  \begin{itemize}
    \item Events are used to track the commands within a queue
    \begin{itemize}
      \item User is able to query the status of an enqueued commands
      \item Status is queued, submitted, running, complete, error.
    \end{itemize}
    \item Event can be also used for intra- and inter-device Communication
    \begin{itemize}
      \item Ensure a status is reached until another command is executed
    \end{itemize}
    \item You can also use the finish command to ensure blocking for a
  \end{itemize}
  \item Intra-Device Communication
  \begin{itemize}
    \item Events are created using asynchronous commands
    \item Events are grouped into event lists
    \item Event lists are used to block a command queue until all events are completed
  \end{itemize}
  \item Inter-Device Communication (Graphic TODO Slide 11 (227))
\end{itemize}

\subsection{Data parallel programming with OpenCL-C}

\begin{itemize}
  \item Programming Model: Single-program multiple-data
  \item Execution Model: Fine-grained data-level parallelism
  \item Memory Model: Memory spaces with different access properties
  \item Prerequsities: What needs to be provided in order to make data-parallel programming worthwile?
  \begin{itemize}
    \item Existence of large data sets which can be broken into smaller data sets
    \item Possibility to operate on the smaller data sets from other sets independently
    \item Hardware architecture which makes a data-parallel execution feasible and preferable
  \end{itemize}
  \item Benefits of data-parallel programming
  \begin{itemize}
    \item Utilization of the architectural resources more efficiently.
    \item Possibility to hide latency of memory accesses in case of computations with high artihmetic intensity (ratio of artithmetic to memory operations)
  \end{itemize}
  \item OpenCL Kernels
  \begin{itemize}
    \item Similar to standard C functions with a set of additional keywords
    \item Kernels contain data-parallel code for parallel execution on a single OpenCL device
    \item Facilitates fine granularity of data-parallelism by allowing to map threads to single data items
    \item Efficient mapping of the kernel code on many devices due to the fine granularity of data-parallelism
  \end{itemize}
  \item OpenCL C-Programming Language
  \begin{itemize}
    \item Open CL-C is based on C99
    \item Special built-in data types (16-bit precision floating point types, vector types, image types, sampler types, API types)
    \item Address Space qualifiers (global: global memory address space, memory objects, local: local memory address space, constant: read only global memory address space, private: variables not declared with an address space qualifier)
    \item Access Qualifiers - Image objects (read_only, write_only, read_write)
    \item Function Qualifiers, e.g. __attribute__((vec_type_hint(<type>)))
    \item Storage Class Qualifiers, e.g. typedef, extern, static
    \item Built-in functions
    \begin{itemize}
      \item Asynchronous Copy \& Prefetching, e.g. async_work_group_copy()
      \item Atomic functions, e.g. atomic_init()
      \item Common functions, e.g. clamp(), min(), max(), sign()
      \item Event functions, e.g. create_user_event()
      \item Geometric functions, e.g. cross(), dot()
      \item Imagine functions, e.g. read_imagef(), get_image_width()
      \item Math functions, e.g. cos(), sin()
      \item Sync functions, e.g. work_group_barrier()
      \item Vector functions, e.g. vload(), vstore()
      \item Work item functions, e.g. get_global_id()
    \end{itemize}
    \item Operators, e.g. +, ++, ==
    \item Conversion and type casting, e.g. convert_int4()
  \end{itemize}
  \item Threads
  \begin{itemize}
    \item Threads may be implemented in software or hardware
    \item A software thread, e.g. as defined in PThreads, can be considered as a lightweight process
    \begin{itemize}
      \item A process, a running program, contains threads which may run concurrently
      \item Threads contain a minimal set of resources (stack) and share the rest (heap) with other threads residing in the process
      \item Faster software context switching organized by the operating system
    \end{itemize}
    \item A hardware thread can be considered as a flow of instructions
    \begin{itemize}
      \item with its program counter and additional registers
      \item and can be switched by the hardware to another hardware thread,
      \item e.g. Intel's Hyperthreading (Simultaneous Multithreading)
    \end{itemize}
    \item Threads in OpenCL
    \begin{itemize}
      \item In OpenCL, a thread is called work-item
      \item Work-items correspond to the smallest unit of a running processes of kernel
      \item A Work-item executes instructions according to the control flow of the kernel body independent of other work-items
      \item Each work-item runs on a unit of hardware, known as a processing element (PE)
      \item A processing element may host many work-items
    \end{itemize}
    \item Thread-Array
    \begin{itemize}
      \item Work-items are organized within a 1D-/2D- or 3D-thread-array
      \item Each work-item has a unique ID within the index get_global_id(dim)
      \item Work-items are grouped to larger entities called work-group, identified by get_group_id(dim)
      \item Within work-groups, work-items are identified by get_local_id(dim).
    \end{itemize}
    \item Indexing the Thread-Array: let $k \in \{0,1,2\}$ specify the domensions $x,y,z$.
    \begin{itemize}
      \item A work-item within a work-group is identified by: $l_k \in \{0,...,L_{k-1}\}$ where $L_k$ is the number of work-items within a work-group.
      \item A work-item within the index space is identified by $f_k \in \{0,...G_{k-1}$ where $G-k$ is the number of work-items within an index space.
      \item A work-group within the index space is identifed by $w_k \in \{0,...W_{k-1}\}$ where $W_k$ is the number of work-groups within an index space.
      \item A work-item then can also be identified by $g_k = l_l + L_k \cdot w_k$ where $L_k \cdot W_k = G_k$
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{OpenCL-Memory}

\begin{itemize}
  \item Prerequsities
  \begin{itemize}
    \item Define index space with global and work-group size for each dimension.
    \item Mostly depends on data set: one work-item $\Leftrightarrow$ one data-item
  \end{itemize}
  \item Kernel Execution
  \begin{itemize}
    \item All work-items execute the kernel: one work-item $Leftrightarrow$ one kernel instance
    \item Any work-items can diverge, i.e. have a different control path than other work-items
    \item Work-item or work-group specific execution: get_global_id(), get_local_id(), get_group_id(), etc.
    \begin{itemize}
      \item the control path e.g. if(get_global_id(0) < elements)
      \item the data path e.g. y[get_global_id(0)] = x[get_global_id(0)]
    \end{itemize}
    \item order of execution is not specified
    \item work-items may be synchronized with barriers work_group_barrier(...)
  \end{itemize}
  \item Warps - Wavefronts
  \begin{itemize}
    \item Work-groups are split into smaller units called warps (Nvidia) or wavefronts (Ati), consisting of 32 or more threads.
    \begin{itemize}
      \item Warps can be considered as hardware threads which execute on a compute unit
      \item Threads within warps execute the same instruction
      \item However, threads within a warp may diverge and take different execution paths
    \end{itemize}
    \item Hardware unit (warp-scheduler) selects warps which are ready to execute on a compute unit in a SIMD fashion
  \end{itemize}
  \item Memory Spaces
  \begin{itemize}
    \item Separate layers of address space which are disjoint
    \item Default address space is for function arguments and local variables private
    \item Pointer arguments to functions are either from global or local memory
  \end{itemize}
  \item Memory Access
  \begin{itemize}
    \item Global (device) is accessible by the host as well as by all work-items of the index space
    \begin{itemize}
      \item Qualified by global within a kernel
      \item Host accesses the device memory via write and read commands
    \end{itemize}
    \item Local memory is accessible by work-items beloging to a work-group (qualified by local within a kernel)
    \item Private memory is only accessible by a work-item
  \end{itemize}
  \item Memory Consistency
  \begin{itemize}
    \item Within a work-item memory has load / store consistency
    \item Local memory is consistent across work-items in a single work-group at a work-group barrier
    \item Global memory is consistent across work-items in a single work-group at a work-group barrier, but there are no guarantees of memory consistency between different work-groups executing a kernel
  \end{itemize}
  \item Code Optimization
  \begin{itemize}
    \item Maximize instruction throughput
    \begin{itemize}
      \item Minimizing the use of instructions with low throughput
      \item Let the thread scheduler to overlap memory transactions with mathematical computations as much as possible
      \item Minimize warp divergence, make the condition dependent on warp size
    \end{itemize}
    \item Maximize memory throughput
    \begin{itemize}
      \item Use local memory if data is reused (caching of the global memory)
      \item Access global memory in coalesced fashion such that consecutive threads access consecutive data-elements in order to issue only one memory transaction
      \item Reduce local memory bank conflicts
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Optimization for CPUs}

\subsection{OpenCL Mapping of CPUs}

\begin{itemize}
  \item AMD OpenCL CPU model
  \item OpenCL CPU model
  \begin{itemize}
    \item Internal realization
    \begin{itemize}
      \item work pool of threads processing kernel
      \item each queue has a core management thread
      \item a work-group is processed in one system thread
      \item work-items in a work-group are processed sequentialy
    \end{itemize}
  \end{itemize}
  \item Intel OpenCL Data-Parallel Execution Model
  \begin{itemize}
    \item Implicit SIMD data parallelism (i.e. shader-style)
    \begin{itemize}
      \item Write the kernel as a scalar program
      \item Use vector data types sized naturally to the algorithm
      \item Kernel automatically mapped to SIMD-compute-resources and cores by the compiler/runtime/hardware
    \end{itemize}
    \item Explicit SIMD data parallelism
    \begin{itemize}
      \item The kernel defines one stream of instructions
      \item Parallelism from source-level wide vector types: size vector types to match native HW width
      \item Programmer hints on the vector data type using attributes: vec_type_hint(typen)
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Optimization Tips}

\begin{itemize}
  \item Kernel code can be imagined as body of for-loops. Every little waste is going to be costly. If you'd move something out of the loop, move it from the kernel.
  \item Avoiding Spurious Operations in Kernel Code - As every line in the kernel is executed many times, make sure you have no spurious instructions in the kernel code. Pass quantities which are constant as parameters.
  \item Performing Initialization in a Separate Task - Moving the initialization phase outside the kernel code, either to a separate kernel, or to the host code. Also if you need to run some kernel exactly once (i.e. just for single work-item) use clEnqueueTask which is specially crafted for this purpose.
  \item Avoiding Handling Edge Conditions in Kernels - Assume you have a full HD image of 1920 x 1080. The four edge if conditions are executed for every pixel, that is, roughly 2 million times. However, they are only relevant for the 6000 pixels on the image edges that make 0.2\% of all the pixels. For the remaining 99.8\% work-items, the edge condition check would be a waste of time.
  \item Using the Preprocessor for Constants - You can use OpenCL dynamic compilation feature to ensure the exponent is known at the kernel compile time (that is still host run time).
  \item Prefer Signed Integer Data Types Over Unsigned - Many image-processing kernels operate on uchar input. To avoid overflow those eight bit input values are typically processed as 16- or 32-bit integer values. In both cases it is strongly recommended to use signed data types (shorts and ints), especially in case of further conversion to floating point and back.
  \item Row-Wise Data Accesses Are Generally Preferable - OpenCL enables you to submit kernel on one-, two- or three-dimensional index space. Given the choice, one-dimensional ranges are preferable, for reasons of cache locality and saving index computations. If two- or three-dimensional range naturally fits your data dimensions, try to keep work-items scanning along rows, not columns.
  \item Trading off Accuracy and Speed of Calculations
  \begin{itemize}
    \item native_* and half_* math built-ins, that have lower precision but are faster than their un-prefixed variants
    \item compiler optimization options like -cl-fast-relaxed-math flag that allows optimizations for floating-point arithmetic for the whole OpenCL program.
  \end{itemize}
  \item Local Memory Usage - One typical GPU-targeted optimization is using local memory for caching of intermediate results. For CPU all OpenCL memory objects are cached by HW, so explicit caching via local memory just introduces unnecessary (moderate) overhead.
  \item Built-In Functions - OpenCL offers a library of built-in functions, including vector variants. Using the built-in functions is typically more efficient than their manual implementation in OpenCL code.
  \item Use Vector Components To Support SIMD usage
  \item Avoid Extracting Vector Components
\end{itemize}

\subsection{Canny Edge Detector}

\begin{itemize}
  \item Gaussian blur filter - Smoothing of image to reduce noise
  \item Compute horizontal derivative with Sobel filter
  \item Compute vertical derivative with Sobel filter
  \item Compute magnitude and angular range
  \item Non-maximum suppression
  \item Determine hysteresis thresholds from image histogram (percentile, median, Otsu)
  \item Hysteresis thresholding
  \item Optimization (see before)
\end{itemize}

\section{Optimization for GPUs}

\subsection{System level performance optimization for CPU-GPU systems with
OpenCL}

\begin{itemize}
  \item Typical image processing pipeline (processing on compute devices (CPUs / GPUs))
  \begin{itemize}
    \item Sensor I/O (Camera / Laser / ...)
    \item Pre-processing (Histogram equalization / Colour adjustment / ...)
    \item Processing (Feature extraction / Object detection / ...)
    \item Post-processing (High level heuristics / Decision logic / ...)
    \item Visualization I/O (Output information / Send to another I/O / ...)
  \end{itemize}
  \item General steps in GPU programming with OpenCL
  \begin{enumerate}
    \item Allocate memory on GPU: Create OpenCL oclBuffer
    \item Queue copy command that copies memory from HOST to GPU: call clEnqueueWriteBuffer
    \item Queue kernel task command: clEnqueueNDRangeKernel
    \item Queue copy command that copies memory from GPU to HOST: call clEnqueueReadBuffer
  \end{enumerate}
  \item Runtime support for memory placement
  \begin{itemize}
    \item Memory management with the GPU driver has a complicated set of memory usage scenarios that need to be considered.
    \item Application developers can control the memory placement by specifying flags during memory allocation as well as through specific memory access or transfer APIs called during runtime.
    \item Application developers need device-specific knowledge in order to know how to avoid these copies.
    \item Creating Buffers with clCreateBuffer() - Memory flags for allocation
    \begin{itemize}
      \item CL_MEM_READ_WRITE, CL_MEM_READ_ONLY, CL_MEM_WRITE_ONLY: Use these flags to allocate a buffer only on the compute device. This buffer matches device memory alignment.
      \item CL_MEM_USE_HOST_PTR: Use this when a buffer is already allocated as page-aligned on the Host.
      \item Create the buffer and its associated cl_mem object using: cl_mem myZeroCopyCLMemObj = clCreateBuffer(ctx, ...CL_MEM_USE_HOST_PTR...);
      \item To avoid extra copies on the device side the host memory needs to follow the compute device memory alignment!
      \item CL_MEM_ALLOC_HOST_PTR: runtime allocates the data on the device with a device specific page-alignment, memory may be read back on the host.
      \item use this flag when you have not yet allocated the memory and want OpenCL to ensure that you have a zero copy buffer: buf = clCreateBuffer(ctx,...CL_MEM_ALLOC_HOST_PTR,...)
      \item Zero-copy memory aka. Pinned memory
      \begin{itemize}
        \item This is host memory that the operating system has bound to a fixed physical address and that the operating system ensures is resident.
        \item The CPU can access pinned host memory at full memory bandwidth.
        \item The runtime limits the total amount of pinned host memory that can be used for memory objects.
        \item If the runtime knows the data is in pinned host memory, it can be transferred to, and from, device memory without requiring staging buffers or having to perform pinning/unpinning on each transfer.
        \item This offers improved transfer performance
      \end{itemize}
      \item CL_MAP_MEM_READ: This flag specifies that the region being mapped in the memory object is being mapped for reading.
      \item CL_MAP_WRITE: This flag specifies that the region being mapped in the memory object is being mapped for writing.
      \item CL_MAP_WRITE_INVALIDATE_REGION
      \begin{itemize}
        \item This flag specifies that the region being mapped in the memory object is being mapped for writing.
        \item This flag alllows the implementation to no longer guarantee that the pointer returned by clEnqueueMapBuffer contains the latest bits in the region being mapped which can be a significant performance enhancement.
        \item It means, if the host/device writes the memory then the device/host can immediately begin with processing without waiting to the end of the transfer.
      \end{itemize}
    \end{itemize}
    \begin{enumerate}
      \item clCreateBuffer(ctx, ...CL_MEM_ALLOC_HOST_PTR, ...)
      \item clMapBuffer(ctx, ...CL_MAP_WRITE_INVALIDATE_REGION, ...)
      \item clEnqueueNDRange(ctx, ...)
    \end{enumerate}
    \item Zero-copy memory is unfortunately a vendor specific OpenCL feature
    \begin{itemize}
      \item The current AMD-runtime supports following setup
      \begin{itemize}
        \item OpenCL runtime use pinned memory for DMA transfer
        \item $\le$ 32 KB: for transfers from the host to device, the data is copied by the CPU to a runtime pinned host memory buffer, then DMA to device
        \item $>$ 32 KB and $\le$ 16 MB: the host memory physcial pages are pinned, then DMA
        \item $>$ 16 MB: OpenCL runtime pins host memory in stages of 16 MB block, then DMA
      \end{itemize}
      \item Data transfer is the key point for system level performance optimization
      \begin{itemize}
        \item Different flags to clCreateBuffer will result in different heap location of the memory object
        \item Different heap location results in different transfer speed
        \item Different memory object type results in different OpenCL runtime overhead
      \end{itemize}
    \end{itemize}
  \end{itemize}
  \item OpenCL data transfers optimization methods
  \begin{itemize}
    \item Till now we have considered a memory placement in the system
    \item Now we take a look on methods that improve data transfers between compute units
    \begin{itemize}
      \item double buffering
      \item Asynchronous transfers with DMA units
    \end{itemize}
    \item With double buffering aka Asynchronous I/O its possible to overleap a computation with data transfers
    \item With two command queues we can reduce N-1 time data movement
  \end{itemize}
  \item Optimizing GPU $Leftrightarrow$ Host communication
  \begin{itemize}
    \item Modern GPUs consist of several DMA (Direct memory access) units
    \item With DMA units GPUs can overlap memory transfers with computation
    \item Dual DMA engines allow Simultaneous bidirectional IO
    \item Possible Improvement with dual DMA engines
    \begin{itemize}
      \item Baseline with one queue = 3 * 5 = 15T
      \item Overlap Case = 7T
      \item Potential-Theoretical Performance Benefit $\approx$ 50 \%
    \end{itemize}
  \end{itemize}
  \item Abstact image processing flow
  \begin{enumerate}
    \item Sensor-Data acquisation
    \item Image processing
    \item Output - visualize
  \end{enumerate}
  \begin{itemize}
    \item Many real-life image processing applications share following process architecture
    \item We analyze the impact of memory placement on the application performance
  \end{itemize}
  \item Recap
  \begin{itemize}
    \item Double buffering should be used to hide the time of data movement
    \item Additionally to reduce unnecessary copies between host and GPU use CL_MEM_ALLOC_HOST_PTR
    \item Benefits of zero-copy (pinned memory):
    \begin{itemize}
      \item Avoid data copies, reduce memory usage
      \item On iGPUs eliminates any transfers via bus
      \item On dGPUs minimize idle times
      \begin{itemize}
        \item Instead of waiting for a host $\Rightarrow$ device memory copy to finish, input data can be processed by the processing units as soon as it arrives.
        \item Instead of waiting for a kernel to complete before initiating a device $\Rightarrow$ host transfer, the data is posted to the bus as soon as the PU are done processing
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Device level (GPU) performance optimization}

\begin{itemize}
  \item Mapping of OpenCL onto GPU Devices
  \begin{itemize}
    \item Work-group maps to a CU
    \item Work-item maps to an ALU
    \item Work-items: Each instance of a Kernel running on a ALU
    \item Work-Group: GPU schedules the range of work-items onto a group of PE, until all work-items have been processed
    \item NDRange: OpenCL maps the total number of work-items to be launched onto a n-dimensional grid, Developers can specify how to divide these work-items into work-groups
  \end{itemize}
  \item SIMT Execution Model
  \begin{itemize}
    \item GPU compute devies are very efficient at parallelizing large numbers of work-items in a manner transparent to the application
    \item Modern GPUs combine work-items in HW-Threads called by: AMD (Wavefront) or NVIDIA (Warps)
    \item Each GPU compute device uses the large number of wavefronts to hide memory access latencies.
    \item The resource scheduler switch the active wavefront in a given compute unit whenever the current wavefronz is stalled (ex. waiting for a memory access to complete)
    \item Hardware multithreading
    \begin{itemize}
      \item Each CU contains a number of wavefronts/warps distributed in the work-groups
      \item The instructions are executed in a lock-step, one instruction for whole HW-thread (warp, wavefront)
      \item The GPU scheduler evaluates all available warps to find one that is ready
      \item This effectively hides latency if there is enough warps to select from and there is a high ratio between memory accesses and arithmetic
    \end{itemize}
  \end{itemize}
  \item Predication and Control Flow
  \begin{itemize}
    \item How do we handle threads going down different execution paths when the same instruction is issued to all the work-items in a wavefront?
    \item Predication is a method for mitigating the costs associated with conditional branches
    \begin{itemize}
      \item Beneficial in case of branches to short sections of code
      \item Based on fact that executing an instruction and squashing its result may be as efficient as executing a conditional
      \item Compilers may replace switch or if then else statements by using branch predication
    \end{itemize}
  \end{itemize}
  \item Predication for GPUs
  \begin{itemize}
    \item Predicate is a condition code that is set to true or false based on a conditional
    \item Both cases of conditional flow get scheduled for execution
    \begin{itemize}
      \item Instructions with a true predicate are commited
      \item Instructions with a false predicate do not write results or read operands
    \end{itemize}
    \item Benefits performance only for very short conditionals
  \end{itemize}
  \item Divergent Control Flow
  \begin{itemize}
    \item Case 1: All odd threads will execute if conditional while all even threads execute the else conditional. the if and else block need to be issued for each wavefront.
    \item Case2: All threads of the first wavefront will execute the if case while other wavefronts will execute the else case. In this case only one out of, if or else is issued for each wavefront.
  \end{itemize}
  \item Occupancy
  \begin{itemize}
    \item Occupancy = active warps / maximum warps per CU
    \item Warps count is controlled by
    \begin{itemize}
      \item work-group sizes
      \item HW-Resources (registers and local memory)
    \end{itemize}
    \item Switchting to other warps enables to mask latency!
    \item Mask instructions latencies is one of key optimization steps!
    \item Enough occupancy depends on the code
    \item To optimize it the programmer need to tune
    \begin{itemize}
      \item work-group sizes
      \item local-shared and register memory usage
    \end{itemize}
  \end{itemize}
  \item GPU Memories
  \begin{itemize}
    \item OpenCL has four memory domains: Private, Local, Constant, Images
    \item Dependent on the HW they present various latencies
  \end{itemize}
  \item GPU Memories - Access pattern
  \begin{itemize}
    \item The discrepancy in execution times between the mappings is due to data accesses on the global memory
    \item Assuming row-major data, data in a row(i.e., elements in adjacent columns) are stored sequentially in memory
    \item To ensure coalesced accesses, consecutive threads in the same wavefronz, HW-threads should be mapped to columns (the second dimension) of the matrices
  \end{itemize}
  \item GPU Global memory access patterns
  \begin{itemize}
    \item Nvidia GPUs
    \begin{itemize}
      \item For example if warp threads accesses elements with the stride = 4 Byte on Nvidia-GPUs and the access is aligned, only one transaction on L1 cache is requested
      \item If sequential threads in a warp access memory that is sequential but not aligned with the cache lines, two 128-byte L1 cache will be requested
      \item On L2 cache segments are only 32-byte wide, so now five 32-byte L2 segments are needed to satisfy the request
    \end{itemize}
  \end{itemize}
  \item GPU Local-shared Memory
  \begin{itemize}
    \item There are many algorithms where the memory access pattern is hard to control, e.g. matrix transpose.
    \item No matter which thread mapping is chosen, one operation (read/write) will produce coalesced accesses while the other (write/read) produces undoalesced accesses
    \item Note that data must be read to a temporary location (such as a register) befire being written to a new location
    \item If local memory is used to buffer the data between reading and writing, we can rearrange the thread mapping to provide coalesced accesses in both directions (Note that the work group must be square)
    \item Local-Shared memory are organized in memory banks
    \item On Nvidia GPUs shared memory uses 32 banks
    \item Dependent on the memory access there are different conflicts
    \item To get best performance programmers need to reduce these conflicts
  \end{itemize}
  \item GPU read-only, constant memories: To map to the read-only memory:
  \begin{itemize}
    \item Annotate pointers with const and __restrict: __kernel void foo(const char* __restrict in) {...} - The compiler knows that the pointers are not aliased
    \item Allocate the memory statically in the OpenCL device Program: int array[100]; __kernel void foo(const char* __restrict in){...}
    \item Use explictly constanct memory space: __kernel void foo(const char* __restrict in, constant char* coef){...}
  \end{itemize}
  \item GPUs texture memories
  \begin{itemize}
    \item C/C++ arrays and OpenCL Buffers objects are stored as a 1D memory objects
    \item GPUs additionally support in hardware the multidimensional memories, 2D images via OpenCL images
    \item OpenCL images are memory object optimized for 2D locality
    \item Adjacent image elements are not guaranteed to be contiguous in HW-memory
    \item Mostly variant of Z curve layout
    \item To allow for hardware abstraction in the physical memory layout, images elements cannot be accessed directly from within the kernel
    \item In OpenCL kernels, the read_image{type} function call is used instead of simply indexing using []
    \item Usage of read_image and write_image is based on hardware functional units accessed via OpenCL samplers
    \item OpenCL Image samplers
    \begin{enumerate}
      \item Pixel Interpolation, either closest pixel can be returned or a linear interpolation (CL_FILTER_NEAREST (no interpolation) or CL_FILTER_LINEAR (linear interpolation))
      \item Variable image coordinates (Integers <0-max_size_int>, Floating <0.0-max_size_float>, Normalized <0.0-1.0>)
      \item Out-of bounds management, hardware manages accesses outside the image (CL_ADDRESS_NONE, CL_ADDRES_CLAMP (set to specified border color value), CL_ADDRES_CLAMP_TO_EDGE (set to the edge color value), CL_ADDRESS_REPEAT (available only with norm. coordinates), CL_ADDRESS_MIRRORED_REPEAT (available only with norm. coordinates))
      \item Channels in OpenCL images refer to the primary colors that make up an image (Each pixel in a texture can contain 1 to 4 channels (R to RGBA), RGBA: red, green, blue, alpha, The color information is sotred as float / integer data, Packaging several values (channels) in a pixel this can improve memory bandwith utilization)
    \end{enumerate}
  \end{itemize}
  \item GPUs extensions
  \begin{itemize}
    \item OpenCL extensions allow for a vendor to expose device functionality without concern for the specification
    \item Different categories of extensions are available
    \begin{itemize}
      \item Khronos/Standard approved
      \begin{itemize}
        \item "cl_khr" in extension names
        \item Approved conformance tests
        \item Might be promoted to required Core feature in later versions
        \item e.g. textensions for atomic operations
      \end{itemize}
      \item External extensions
      \begin{itemize}
        \item "cl_ext" in extension name
        \item Developed by 2 or more members of the working group
        \item No required conformance tests
        \item e.q. Device Fission (cl_ext_device_fission)
      \end{itemize}
      \item Vendor specific ext.
      \begin{itemize}
        \item Developed by a single vendor
        \item e.g. AMD cl_amd_printf
      \end{itemize}
    \end{itemize}
    \item Enabling extensions is done in host code by developer - Always check for availability of required extensions before running program in order to maintain device compatibility
    \item OpenCL extensions have to be enabled in kernel code
    \item Intial state: all extensions disabled
    \item Error and warning reporting done according to specification
    \item Programmer's responsiblity to specify what extensions his code needs
    \item Known target device will not be known till runtime
    \item Check device and possibly have a fall-back version because code using any extension will compile as long as the pragma is added to the kernel
    \item Application can query device for information about extensions using CL_DEVICE_EXTENSIONS parameter
    \item Checking for extensions
    \begin{itemize}
      \item Steps to check for the availability of an extension
      \item Query device using CL_DEVICE_EXTENSIONS
      \item Names of extensions supported by device returned in a character array
      \item Search in array for required extension
    \end{itemize}
    \item OpenCL supports wide group of instructions
    \item Note: various support for matah. Instructions (half, single, double precisions)
  \end{itemize}
  \item Half, single and double precision extensions
  \begin{itemize}
    \item OpenCL provides double precision floating-point as an optional extension (cl_khr_fp64)
    \item Standard: single precision for floating point ops.
    \item Enable extension by using directive in kernel file: double precision vectors of type double, single precisionxs vectors of type float
    \item Half precision is defined using the extension cl_khr_fp16
    \item Really important optimization, high possible speed-ups!
  \end{itemize}
  \item OpenCL atomic instructions
  \begin{itemize}
    \item int atom_add(__global/_local int *p, int val) - Atomically add val to data at location pointed to by p and return old value
    \item int atom_sub(__global/_local int *p, int val) - Atomically subtract val to data at location pointed to by p and return old value
    \item int atom_xchg(__global/_local int *p, int val) - Swaps the old value pointed to by p with val
    \item int atom_inc(__global/_local int* p) - Atomically increment the 32-bit value at location pointed by p. Return the old value
    \item int atom_dec(__global/_local int* p) - Atomically decrement the 32-bit value at location pointed by p. Return the old value
    \item int_atom_cmpxchg(__global int* p, int cmp, int val) - Compare val with data at location p and exchange if not equal. Return old value
    \item int atom_min(__global int *p, int val) - Store min(val, *p) at location pointed by p. Returns original value
    \item int atom_max(__global int *p, int val) - Store max(val, *p) at location pointed to by p. Returns original value
    \item int atom_and(__global int *p, int val) - Store and(val, *p) at location pointed to by p. Returns original value
    \item int atom_or(__global int *p, int val) - Store or(val, *p) at location pointed to by p. Returns original value
    \item int atom_xor(__global int *p, int val) - Store xor(val, *p) at location pointed to by p. Returns original value
  \end{itemize}
  \item Vendor specific extension
  \begin{itemize}
    \item AMD: cl_amd_media_ops, cl_amd_media_ops2, cl_amd_bus_addressable_memory, ....
    \item ARM: cl_arm_dot_product, cl_arm_shared_virtual_memory, ...
    \item Intel: cl_intel_accelerator, cl_intel_motion_estimation, cl_intel_subgroups, ...
  \end{itemize}
  \item Recap
  \begin{itemize}
    \item Latency optimization: Occupancy maximize, thread divergence minimize
    \item Memory throughput optimizations: Global, local, constant, private texture memories accesses
    \item Instruction throughput optimizations: Math, atomic, extensions
  \end{itemize}
\end{itemize}

\section{Optimization GPU programming}

\begin{itemize}
  \item Application tuning process
  \begin{itemize}
    \item START
    \item Chose an efficient algorithm
    \item Write the code
    \item Optimize to increase efficiency
    \item Validate the results
    \item Check performance
    \item If not efficient, try to optimize further or change the algorithm
    \item If efficient - READY
  \end{itemize}
\end{itemize}

\end{document}
